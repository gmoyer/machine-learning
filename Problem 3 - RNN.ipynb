{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this problem we will create a NN to write Shakespeare\n",
    "# plays.\n",
    "# The training data is included in the Training Data \n",
    "# subfolder, and was taken from here:\n",
    "# https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "#\n",
    "# The idea for the project came from this awesome blog post:\n",
    "# https://karpathy.github.io/2015/05/21/rnn-effectiveness/ \n",
    "# I highly recommend you read it. It is great.\n",
    "\n",
    "# Packages\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "### -------- Import Data and Data Preprocessing -------- ###\n",
    "# you must include the appropriate data preprocessing steps\n",
    "\n",
    "# Load the data\n",
    "with open('Training Data/3-RNN_input.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "chars = list(set(data))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {char: index for index, char in int2char.items()}\n",
    "data = torch.tensor([char2int[char] for char in data])\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the data is: 65\n",
      "This helps me set up the output of the data, as there need to be 65 choices\n",
      "The length of the data is: 1115394\n",
      "I can choose a rather long sequence to learn from because I have a lot of data\n"
     ]
    }
   ],
   "source": [
    "### ------------ Exploratory Data Analysis ------------- ###\n",
    "# Output two pieces of information that you found \n",
    "# informative as well as a print statement of why they\n",
    "# assisted you in choosing your model parameters\n",
    "\n",
    "print('The number of unique characters in the data is:', len(chars))\n",
    "print('This helps me set up the output of the data, as there need to be 65 choices')\n",
    "\n",
    "print('The length of the data is:', len(data))\n",
    "print('I can choose a rather long sequence to learn from because I have a lot of data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------- Model Definition ------------------ ###\n",
    "# Use an LSTM\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, len(chars))\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        self.hidden = (torch.zeros(2, 1, self.hidden_size), torch.zeros(2, 1, self.hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.hidden = self.init_hidden()\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.linear(x)\n",
    "        return x[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm using Adam because it is the best optimizer for most problems\n",
      "Since we are doing a multivariate classification, cross entropy loss is a good choice\n",
      "It also does the one-hot encoding for us\n"
     ]
    }
   ],
   "source": [
    "### --------- Optimizer and Loss Definition ------------ ###\n",
    "# Output a print statement supporting your optimizer and \n",
    "# loss function choices\n",
    "\n",
    "model = Model(hidden_size=512, num_layers=3)\n",
    "sequence_length = 50\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"I'm using Adam because it is the best optimizer for most problems\")\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "print(\"Since we are doing a multivariate classification, cross entropy loss is a good choice\")\n",
    "print(\"It also does the one-hot encoding for us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------ Batch Setup --------------------- ###\n",
    "\n",
    "data_order = torch.randperm(len(data) - sequence_length)\n",
    "def batches(batch_size=10):\n",
    "    for i in range(0, len(data_order), batch_size):\n",
    "        start = data_order[i:i+batch_size]\n",
    "        end = start + sequence_length\n",
    "        \n",
    "        x = torch.stack([data[start:end] for start, end in zip(start, end)]).view(-1, sequence_length, 1)\n",
    "        y = torch.stack([data[end] for end in end])\n",
    "        x = x.float()\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss: 4.168362140655518\n",
      "Epoch 1 Batch 1000 Loss: 2.722783088684082\n",
      "Epoch 1 Batch 2000 Loss: 2.526240110397339\n",
      "Epoch 1 Batch 3000 Loss: 2.0892553329467773\n",
      "Epoch 1 Batch 4000 Loss: 1.991926670074463\n",
      "Epoch 1 Batch 5000 Loss: 1.8739547729492188\n",
      "Epoch 1 Batch 6000 Loss: 1.8590413331985474\n",
      "Epoch 1 Batch 7000 Loss: 1.7220540046691895\n",
      "Epoch 1 Batch 8000 Loss: 1.71344792842865\n",
      "Epoch 1 Batch 9000 Loss: 1.8991285562515259\n",
      "Epoch 1 Batch 10000 Loss: 1.8734818696975708\n",
      "Epoch 1 Batch 11000 Loss: 1.8212858438491821\n",
      "Epoch 1 completed\n",
      "\n",
      "Epoch 2 Batch 0 Loss: 1.7796648740768433\n",
      "Epoch 2 Batch 1000 Loss: 1.818304419517517\n",
      "Epoch 2 Batch 2000 Loss: 1.908031940460205\n",
      "Epoch 2 Batch 3000 Loss: 1.6090906858444214\n",
      "Epoch 2 Batch 4000 Loss: 1.6651911735534668\n",
      "Epoch 2 Batch 5000 Loss: 1.5445263385772705\n",
      "Epoch 2 Batch 6000 Loss: 1.6057806015014648\n",
      "Epoch 2 Batch 7000 Loss: 1.4871100187301636\n",
      "Epoch 2 Batch 8000 Loss: 1.473750352859497\n",
      "Epoch 2 Batch 9000 Loss: 1.7852187156677246\n",
      "Epoch 2 Batch 10000 Loss: 1.6981619596481323\n",
      "Epoch 2 Batch 11000 Loss: 1.6630216836929321\n",
      "Epoch 2 completed\n",
      "\n",
      "Epoch 3 Batch 0 Loss: 1.6335166692733765\n",
      "Epoch 3 Batch 1000 Loss: 1.664858341217041\n",
      "Epoch 3 Batch 2000 Loss: 1.6872440576553345\n",
      "Epoch 3 Batch 3000 Loss: 1.4177193641662598\n",
      "Epoch 3 Batch 4000 Loss: 1.585023045539856\n",
      "Epoch 3 Batch 5000 Loss: 1.4226088523864746\n",
      "Epoch 3 Batch 6000 Loss: 1.5411344766616821\n",
      "Epoch 3 Batch 7000 Loss: 1.4016269445419312\n",
      "Epoch 3 Batch 8000 Loss: 1.3401528596878052\n",
      "Epoch 3 Batch 9000 Loss: 1.763445258140564\n",
      "Epoch 3 Batch 10000 Loss: 1.5592939853668213\n",
      "Epoch 3 Batch 11000 Loss: 1.614342451095581\n",
      "Epoch 3 completed\n",
      "\n",
      "Epoch 4 Batch 0 Loss: 1.5141618251800537\n",
      "Epoch 4 Batch 1000 Loss: 1.5705140829086304\n",
      "Epoch 4 Batch 2000 Loss: 1.5970687866210938\n",
      "Epoch 4 Batch 3000 Loss: 1.3080295324325562\n",
      "Epoch 4 Batch 4000 Loss: 1.5063928365707397\n",
      "Epoch 4 Batch 5000 Loss: 1.3235909938812256\n",
      "Epoch 4 Batch 6000 Loss: 1.5275709629058838\n",
      "Epoch 4 Batch 7000 Loss: 1.3580567836761475\n",
      "Epoch 4 Batch 8000 Loss: 1.3147306442260742\n",
      "Epoch 4 Batch 9000 Loss: 1.7897365093231201\n",
      "Epoch 4 Batch 10000 Loss: 1.4907886981964111\n",
      "Epoch 4 Batch 11000 Loss: 1.5001825094223022\n",
      "Epoch 4 completed\n",
      "\n",
      "Epoch 5 Batch 0 Loss: 1.4652290344238281\n",
      "Epoch 5 Batch 1000 Loss: 1.549443244934082\n",
      "Epoch 5 Batch 2000 Loss: 1.5327640771865845\n",
      "Epoch 5 Batch 3000 Loss: 1.2525559663772583\n",
      "Epoch 5 Batch 4000 Loss: 1.4007630348205566\n",
      "Epoch 5 Batch 5000 Loss: 1.3103208541870117\n",
      "Epoch 5 Batch 6000 Loss: 1.4822256565093994\n",
      "Epoch 5 Batch 7000 Loss: 1.3169959783554077\n",
      "Epoch 5 Batch 8000 Loss: 1.2363964319229126\n",
      "Epoch 5 Batch 9000 Loss: 1.6894638538360596\n",
      "Epoch 5 Batch 10000 Loss: 1.4172645807266235\n",
      "Epoch 5 Batch 11000 Loss: 1.4540899991989136\n",
      "Epoch 5 completed\n",
      "\n",
      "Epoch 6 Batch 0 Loss: 1.4259976148605347\n",
      "Epoch 6 Batch 1000 Loss: 1.577020525932312\n",
      "Epoch 6 Batch 2000 Loss: 1.485091209411621\n",
      "Epoch 6 Batch 3000 Loss: 1.266849398612976\n",
      "Epoch 6 Batch 4000 Loss: 1.3593555688858032\n",
      "Epoch 6 Batch 5000 Loss: 1.3602957725524902\n",
      "Epoch 6 Batch 6000 Loss: 1.4686288833618164\n",
      "Epoch 6 Batch 7000 Loss: 1.297571063041687\n",
      "Epoch 6 Batch 8000 Loss: 1.1863338947296143\n",
      "Epoch 6 Batch 9000 Loss: 1.7179746627807617\n",
      "Epoch 6 Batch 10000 Loss: 1.4189605712890625\n",
      "Epoch 6 Batch 11000 Loss: 1.4884464740753174\n",
      "Epoch 6 completed\n",
      "\n",
      "Epoch 7 Batch 0 Loss: 1.3836543560028076\n",
      "Epoch 7 Batch 1000 Loss: 1.4966671466827393\n",
      "Epoch 7 Batch 2000 Loss: 1.429425835609436\n",
      "Epoch 7 Batch 3000 Loss: 1.2329078912734985\n",
      "Epoch 7 Batch 4000 Loss: 1.40337336063385\n",
      "Epoch 7 Batch 5000 Loss: 1.2535529136657715\n",
      "Epoch 7 Batch 6000 Loss: 1.4347069263458252\n",
      "Epoch 7 Batch 7000 Loss: 1.268323540687561\n",
      "Epoch 7 Batch 8000 Loss: 1.1668787002563477\n",
      "Epoch 7 Batch 9000 Loss: 1.6708563566207886\n",
      "Epoch 7 Batch 10000 Loss: 1.4158352613449097\n",
      "Epoch 7 Batch 11000 Loss: 1.3941371440887451\n",
      "Epoch 7 completed\n",
      "\n",
      "Epoch 8 Batch 0 Loss: 1.4484573602676392\n",
      "Epoch 8 Batch 1000 Loss: 1.4136319160461426\n",
      "Epoch 8 Batch 2000 Loss: 1.3819621801376343\n",
      "Epoch 8 Batch 3000 Loss: 1.1956379413604736\n",
      "Epoch 8 Batch 4000 Loss: 1.3450443744659424\n",
      "Epoch 8 Batch 5000 Loss: 1.3093664646148682\n",
      "Epoch 8 Batch 6000 Loss: 1.4359314441680908\n",
      "Epoch 8 Batch 7000 Loss: 1.2436329126358032\n",
      "Epoch 8 Batch 8000 Loss: 1.1883327960968018\n",
      "Epoch 8 Batch 9000 Loss: 1.6493486166000366\n",
      "Epoch 8 Batch 10000 Loss: 1.3927741050720215\n",
      "Epoch 8 Batch 11000 Loss: 1.3328160047531128\n",
      "Epoch 8 completed\n",
      "\n",
      "Epoch 9 Batch 0 Loss: 1.4196724891662598\n",
      "Epoch 9 Batch 1000 Loss: 1.5001559257507324\n",
      "Epoch 9 Batch 2000 Loss: 1.3137562274932861\n",
      "Epoch 9 Batch 3000 Loss: 1.1495896577835083\n",
      "Epoch 9 Batch 4000 Loss: 1.2744550704956055\n",
      "Epoch 9 Batch 5000 Loss: 1.2891147136688232\n",
      "Epoch 9 Batch 6000 Loss: 1.4005001783370972\n",
      "Epoch 9 Batch 7000 Loss: 1.317492961883545\n",
      "Epoch 9 Batch 8000 Loss: 1.1816684007644653\n",
      "Epoch 9 Batch 9000 Loss: 1.600085735321045\n",
      "Epoch 9 Batch 10000 Loss: 1.3189735412597656\n",
      "Epoch 9 Batch 11000 Loss: 1.3446934223175049\n",
      "Epoch 9 completed\n",
      "\n",
      "Epoch 10 Batch 0 Loss: 1.3798604011535645\n",
      "Epoch 10 Batch 1000 Loss: 1.3668516874313354\n",
      "Epoch 10 Batch 2000 Loss: 1.3543542623519897\n",
      "Epoch 10 Batch 3000 Loss: 1.1451163291931152\n",
      "Epoch 10 Batch 4000 Loss: 1.3137643337249756\n",
      "Epoch 10 Batch 5000 Loss: 1.1803534030914307\n",
      "Epoch 10 Batch 6000 Loss: 1.3739039897918701\n",
      "Epoch 10 Batch 7000 Loss: 1.249649167060852\n",
      "Epoch 10 Batch 8000 Loss: 1.1382390260696411\n",
      "Epoch 10 Batch 9000 Loss: 1.553889513015747\n",
      "Epoch 10 Batch 10000 Loss: 1.3082292079925537\n",
      "Epoch 10 Batch 11000 Loss: 1.3261831998825073\n",
      "Epoch 10 completed\n",
      "\n",
      "Epoch 11 Batch 0 Loss: 1.3651574850082397\n",
      "Epoch 11 Batch 1000 Loss: 1.3148183822631836\n",
      "Epoch 11 Batch 2000 Loss: 1.3384718894958496\n",
      "Epoch 11 Batch 3000 Loss: 1.1915311813354492\n",
      "Epoch 11 Batch 4000 Loss: 1.3185511827468872\n",
      "Epoch 11 Batch 5000 Loss: 1.2268078327178955\n",
      "Epoch 11 Batch 6000 Loss: 1.4634220600128174\n",
      "Epoch 11 Batch 7000 Loss: 1.299095630645752\n",
      "Epoch 11 Batch 8000 Loss: 1.1406660079956055\n",
      "Epoch 11 Batch 9000 Loss: 1.5827043056488037\n",
      "Epoch 11 Batch 10000 Loss: 1.2834950685501099\n",
      "Epoch 11 Batch 11000 Loss: 1.3522660732269287\n",
      "Epoch 11 completed\n",
      "\n",
      "Epoch 12 Batch 0 Loss: 1.3480849266052246\n",
      "Epoch 12 Batch 1000 Loss: 1.3875318765640259\n",
      "Epoch 12 Batch 2000 Loss: 1.324613094329834\n",
      "Epoch 12 Batch 3000 Loss: 1.0703577995300293\n",
      "Epoch 12 Batch 4000 Loss: 1.2409656047821045\n",
      "Epoch 12 Batch 5000 Loss: 1.2099123001098633\n",
      "Epoch 12 Batch 6000 Loss: 1.3621264696121216\n",
      "Epoch 12 Batch 7000 Loss: 1.2444952726364136\n",
      "Epoch 12 Batch 8000 Loss: 1.1277897357940674\n",
      "Epoch 12 Batch 9000 Loss: 1.5414563417434692\n",
      "Epoch 12 Batch 10000 Loss: 1.2650741338729858\n",
      "Epoch 12 Batch 11000 Loss: 1.3367211818695068\n",
      "Epoch 12 completed\n",
      "\n",
      "Epoch 13 Batch 0 Loss: 1.3912655115127563\n",
      "Epoch 13 Batch 1000 Loss: 1.3813363313674927\n",
      "Epoch 13 Batch 2000 Loss: 1.3162826299667358\n",
      "Epoch 13 Batch 3000 Loss: 1.1370457410812378\n",
      "Epoch 13 Batch 4000 Loss: 1.265562891960144\n",
      "Epoch 13 Batch 5000 Loss: 1.2139897346496582\n",
      "Epoch 13 Batch 6000 Loss: 1.3950544595718384\n",
      "Epoch 13 Batch 7000 Loss: 1.2268537282943726\n",
      "Epoch 13 Batch 8000 Loss: 1.1137067079544067\n",
      "Epoch 13 Batch 9000 Loss: 1.4921902418136597\n",
      "Epoch 13 Batch 10000 Loss: 1.3262022733688354\n",
      "Epoch 13 Batch 11000 Loss: 1.2538868188858032\n",
      "Epoch 13 completed\n",
      "\n",
      "Epoch 14 Batch 0 Loss: 1.3094123601913452\n",
      "Epoch 14 Batch 1000 Loss: 1.405299425125122\n",
      "Epoch 14 Batch 2000 Loss: 1.329768180847168\n",
      "Epoch 14 Batch 3000 Loss: 1.1686581373214722\n",
      "Epoch 14 Batch 4000 Loss: 1.2733337879180908\n",
      "Epoch 14 Batch 5000 Loss: 1.2039172649383545\n",
      "Epoch 14 Batch 6000 Loss: 1.3443855047225952\n",
      "Epoch 14 Batch 7000 Loss: 1.2246590852737427\n",
      "Epoch 14 Batch 8000 Loss: 1.087824821472168\n",
      "Epoch 14 Batch 9000 Loss: 1.5379687547683716\n",
      "Epoch 14 Batch 10000 Loss: 1.2777204513549805\n",
      "Epoch 14 Batch 11000 Loss: 1.2845823764801025\n",
      "Epoch 14 completed\n",
      "\n",
      "Epoch 15 Batch 0 Loss: 1.3565672636032104\n",
      "Epoch 15 Batch 1000 Loss: 1.3068655729293823\n",
      "Epoch 15 Batch 2000 Loss: 1.2817158699035645\n",
      "Epoch 15 Batch 3000 Loss: 1.1923859119415283\n",
      "Epoch 15 Batch 4000 Loss: 1.1809183359146118\n",
      "Epoch 15 Batch 5000 Loss: 1.1874035596847534\n",
      "Epoch 15 Batch 6000 Loss: 1.397912621498108\n",
      "Epoch 15 Batch 7000 Loss: 1.262908935546875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, y)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\student\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### ---------------- Training pt I --------------------- ###\n",
    "# Train 10 epochs\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_num = 0\n",
    "    for x, y in batches(batch_size=100):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "\n",
    "        loss = loss_function(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 1000 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch_num} Loss: {loss}')\n",
    "\n",
    "        batch_num += 1\n",
    "    print(f'Epoch {epoch+1} completed\\n')\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'rnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------------- Testing pt I ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt1.pdf\"\n",
    "\n",
    "# model.load_state_dict(torch.load(\"rnn_model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "model.init_hidden()\n",
    "output = []\n",
    "x = torch.tensor([char2int[char] for char in 'The ']).view(1, -1, 1).float()\n",
    "for i in range(2000):\n",
    "    y = model(x)\n",
    "    prediction = torch.argmax(y, dim=1)\n",
    "    output.append(int2char[prediction.item()])\n",
    "    x = torch.cat([x, prediction.view(1, 1, 1).float()], dim=1)[:, -sequence_length:, :]\n",
    "\n",
    "output = ''.join(output)\n",
    "\n",
    "with open('RNN_pt3.txt', 'w') as file:\n",
    "    file.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Training pt II --------------------- ###\n",
    "# Train an ADDITIONAL 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------------- Testing pt II ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------- Training pt III --------------------- ###\n",
    "# Train until you can get it to write a good essay. Take\n",
    "# advantage of the fact that pytorch doesn't reset your model\n",
    "# unless you reinstantiate it in the \"Model Definition\" cell\n",
    "#\n",
    "# If after 3 hours it still doesn't make a meaningful essay\n",
    "# then change some hyperparameters and try again. You can \n",
    "# look to the blog post for hyperparameter inspiration.\n",
    "\n",
    "print(\"I was only able to train 15 epochs as it took 48 hours to do so.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### -------------- Testing pt III ---------------------- ###\n",
    "# Write an essay with a minimum of 2,000 characters and \n",
    "# save the output as a PDF named \"RNN_pt3.pdf\"\n",
    "#\n",
    "# Output a print statement commenting on wether or not you\n",
    "# enjoyed this problem and why or why not.\n",
    "\n",
    "print(\"I am happy with the output of my network, but I believe that it took to long to get there and took too much processing power and energy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
